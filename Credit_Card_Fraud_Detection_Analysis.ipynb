{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d494951",
   "metadata": {},
   "source": [
    "\n",
    "# Credit Card Fraud Detection Analysis\n",
    "\n",
    "## 1. Introduction and Brief Description of the Dataset\n",
    "\n",
    "In this analysis, we explore the **Credit Card Fraud Detection** dataset from OpenML (https://www.openml.org/d/1597). The dataset contains anonymized credit card transaction data from European cardholders, collected over two days in September 2013. The goal is to predict whether a given transaction is fraudulent or not.\n",
    "\n",
    "**Dataset Features**:\n",
    "- **Time**: Number of seconds elapsed between this transaction and the first transaction in the dataset.\n",
    "- **Amount**: Transaction amount.\n",
    "- **V1-V28**: Principal Component Analysis (PCA) transformed features.\n",
    "- **Class**: Binary target, where 1 indicates a fraudulent transaction, and 0 indicates a legitimate transaction.\n",
    "\n",
    "The dataset is highly imbalanced, with only 0.1727% of transactions marked as fraudulent. Handling this imbalance is a critical aspect of the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a52168",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset from OpenML\n",
    "url = 'https://www.openml.org/data/get_csv/1597/dataset_1597_creditcard.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the dataset for initial inspection\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d44979",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Objectives\n",
    "\n",
    "The main objective of this analysis is to create a classification model that can effectively distinguish between fraudulent and non-fraudulent transactions. We aim to identify fraudulent transactions with high precision while minimizing false negatives, as missing a fraudulent transaction is more costly than incorrectly classifying a legitimate transaction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c203755",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Model Selection and Training\n",
    "\n",
    "We will train three classification models:\n",
    "1. **Logistic Regression**: A baseline model to provide a simple benchmark.\n",
    "2. **Random Forest Classifier**: An ensemble model that can improve prediction performance.\n",
    "3. **Gradient Boosting Classifier**: Another ensemble model known for its effectiveness in imbalanced datasets.\n",
    "\n",
    "We will start by preparing the data and handling categorical variables where needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42fea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e0f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First Model: Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_log_reg = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Model evaluation\n",
    "log_reg_report = classification_report(y_test, y_pred_log_reg)\n",
    "log_reg_cm = confusion_matrix(y_test, y_pred_log_reg)\n",
    "\n",
    "log_reg_report, log_reg_cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06823c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Second Model: Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_clf.predict(X_test_scaled)\n",
    "\n",
    "# Model evaluation\n",
    "rf_report = classification_report(y_test, y_pred_rf)\n",
    "rf_cm = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "rf_report, rf_cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5055f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Third Model: Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "gb_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_gb = gb_clf.predict(X_test_scaled)\n",
    "\n",
    "# Model evaluation\n",
    "gb_report = classification_report(y_test, y_pred_gb)\n",
    "gb_cm = confusion_matrix(y_test, y_pred_gb)\n",
    "\n",
    "gb_report, gb_cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb9aa72",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Insights and Key Findings\n",
    "\n",
    "The results of the models are summarized below:\n",
    "\n",
    "- **Logistic Regression**: A simple model with moderate performance, struggling with detecting fraudulent transactions.\n",
    "- **Random Forest Classifier**: Shows improvement in precision but still lacks recall for detecting fraudulent cases.\n",
    "- **Gradient Boosting Classifier**: Provides the best balance between precision and recall, but further improvements are possible.\n",
    "\n",
    "Based on these results, we recommend using the Gradient Boosting Classifier for this dataset, as it provides the best balance between the conflicting metrics of precision and recall for fraudulent transactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fc879",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Next Steps\n",
    "\n",
    "Here are some steps to improve the model's performance:\n",
    "\n",
    "1. **Handling Class Imbalance**: Implement methods such as SMOTE (Synthetic Minority Over-sampling Technique) to handle class imbalance.\n",
    "2. **Hyperparameter Tuning**: Use techniques like GridSearchCV to fine-tune hyperparameters, particularly for the Random Forest and Gradient Boosting models.\n",
    "3. **Feature Engineering**: Explore feature creation or selection methods to improve model performance.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
