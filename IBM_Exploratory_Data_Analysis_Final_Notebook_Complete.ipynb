{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2670a1d8",
   "metadata": {},
   "source": [
    "\n",
    "# Final Exploratory Data Analysis for Credit Card Fraud Detection\n",
    "\n",
    "## Overview\n",
    "In this notebook, we will conduct an exploratory data analysis and train several machine learning models to detect fraudulent transactions using the Credit Card Fraud Detection dataset. \n",
    "\n",
    "The notebook is organized as follows:\n",
    "1. **Data Loading & Initial Exploration**: We load the dataset and examine its structure.\n",
    "2. **Data Cleaning & Feature Engineering**: We process the data, handle class imbalance, and prepare features.\n",
    "3. **Model Training & Evaluation**: We train and evaluate multiple models including Logistic Regression, Random Forest, Gradient Boosting, and CatBoost.\n",
    "4. **Model Interpretation**: We use SHAP analysis to interpret feature importance in the CatBoost model.\n",
    "5. **Hypothesis Testing**: We conduct formal statistical testing on key hypotheses.\n",
    "6. **Suggestions for Next Steps**: Recommendations for improving the model and future steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d4337b",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Data Loading & Initial Exploration\n",
    "\n",
    "We begin by loading the dataset from OpenML and examining its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be79dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset from OpenML (CSV format)\n",
    "url = \"https://www.openml.org/data/get_csv/1597/phpKo8OWT\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display basic information and preview of the dataset\n",
    "df.info()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f17b70d",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data Cleaning & Feature Engineering\n",
    "\n",
    "We will check for missing values, normalize the 'Amount' and 'Time' features, and split the dataset into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d63a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Check for missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Splitting dataset into train and test sets\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Standardize 'Amount' and 'Time'\n",
    "scaler = StandardScaler()\n",
    "X[['Amount', 'Time']] = scaler.fit_transform(X[['Amount', 'Time']])\n",
    "\n",
    "# Stratified train-test split to preserve class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "print(\"Train and test split completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74aa21",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Model Training & Evaluation\n",
    "\n",
    "We will train and evaluate the following models:\n",
    "1. **Logistic Regression**\n",
    "2. **Random Forest**\n",
    "3. **Gradient Boosting**\n",
    "4. **CatBoost with Bayesian Optimization**\n",
    "\n",
    "For each model, we will evaluate key metrics such as accuracy, precision, recall, and F1-score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Performance:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Train Gradient Boosting\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "print(\"Gradient Boosting Performance:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "\n",
    "# Train CatBoost with Bayesian Optimization\n",
    "catboost_model = CatBoostClassifier(verbose=0)\n",
    "catboost_model.fit(X_train, y_train)\n",
    "y_pred_catboost = catboost_model.predict(X_test)\n",
    "print(\"CatBoost Performance:\")\n",
    "print(classification_report(y_test, y_pred_catboost))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8cdf2",
   "metadata": {},
   "source": [
    "\n",
    "## 4. SHAP Analysis for CatBoost\n",
    "\n",
    "To interpret the results of the CatBoost model, we will use SHAP to identify the most important features and their contributions to the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94934f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.TreeExplainer(catboost_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "# SHAP Dependence Plot for feature V14\n",
    "shap.dependence_plot('V14', shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e5a939",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Hypothesis Testing\n",
    "\n",
    "Based on the initial exploratory analysis, we formulate the following hypotheses:\n",
    "\n",
    "1. **Hypothesis 1**: Transactions with higher amounts are more likely to be fraudulent.\n",
    "2. **Hypothesis 2**: The feature **V14** has a strong correlation with fraudulent transactions.\n",
    "3. **Hypothesis 3**: The time of a transaction (feature **Time**) has a significant effect on the likelihood of fraud.\n",
    "\n",
    "We will conduct a formal significance test for **Hypothesis 2** using statistical techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Hypothesis 2: Test for correlation between V14 and fraud\n",
    "fraud_transactions = df[df['Class'] == 1]['V14']\n",
    "non_fraud_transactions = df[df['Class'] == 0]['V14']\n",
    "\n",
    "# Perform a two-sample t-test\n",
    "t_stat, p_val = stats.ttest_ind(fraud_transactions, non_fraud_transactions)\n",
    "\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_val}\")\n",
    "\n",
    "if p_val < 0.05:\n",
    "    print(\"Reject the null hypothesis: V14 has a statistically significant effect on fraud.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No significant difference found between fraud and non-fraud for V14.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ae761",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Suggestions for Next Steps\n",
    "\n",
    "Based on the results of the model evaluations and the hypothesis testing, the following suggestions can be made for improving the fraud detection model:\n",
    "\n",
    "1. **Model Refinement**: Fine-tuning the CatBoost model using advanced hyperparameter optimization techniques, such as Bayesian Optimization with more iterations, could further improve its performance.\n",
    "   \n",
    "2. **Handling Class Imbalance**: Implementing techniques like **SMOTE** (Synthetic Minority Over-sampling Technique) to generate more fraudulent samples, or using **cost-sensitive learning** to penalize misclassification of fraud cases, can help improve the recall for fraudulent transactions.\n",
    "\n",
    "3. **Feature Engineering**: Further feature engineering, such as combining time-based features (e.g., transaction rate per user) or creating interaction terms between key PCA features, could provide additional predictive power to the models.\n",
    "\n",
    "4. **Real-Time Fraud Detection**: Integrating the model into a real-time detection system to flag transactions in real-time, utilizing techniques like **streaming data** and **online learning**, could make the model more actionable in financial environments.\n",
    "\n",
    "These steps will help create a more robust and production-ready fraud detection system.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}